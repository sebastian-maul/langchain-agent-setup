# Example model_config.yaml for Ollama integration with AutoGen
# See https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html#ollama for details

models:
  - model: ollama
    model_server: http://localhost:11434
    model_name: granite3.3:8b
    temperature: 0.1
    max_tokens: 2048
    request_timeout: 120
    # Add any other Ollama-specific parameters here
